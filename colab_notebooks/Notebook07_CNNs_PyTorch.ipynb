{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](http://colab.research.google.com/github/ai2es/WAF_ML_Tutorial_Part2/blob/main/colab_notebooks/Notebook07_CNNs.ipynb)\n",
    "\n",
    "\n",
    "\n",
    " # Notebook 07: Convolutional Neural Networks\n",
    "\n",
    "\n",
    "\n",
    " ### Primary Goal:\n",
    "\n",
    "\n",
    "\n",
    " Build and train a convolutional neural network\n",
    "\n",
    "\n",
    "\n",
    " #### Background\n",
    "\n",
    "\n",
    "\n",
    " Now that we have thoroughly discussed what a convolution is, here we will dive into training a convolutional neural network on the ```sub-sevir``` dataset.\n",
    "\n",
    "\n",
    "\n",
    " #### Step 0\n",
    "\n",
    "\n",
    "\n",
    " As with all of our notebooks, we will first install packages and grab data\n",
    "\n",
    "\n",
    "\n",
    " Note: downloading the data can take up to 15-20 mins depending on the server speed. Please be patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Install and unpack files [this will take some time]\n",
    "\n",
    "print('installing dependancies')\n",
    "!pip -q install zenodo-get\n",
    "!pip -q install zarr \n",
    "print('grabbing sub-sevir')\n",
    "!zenodo_get 7011372\n",
    "print('unpacking sub-sevir')\n",
    "!tar -xf sub-sevir.tar.gz\n",
    "!tar -xf sub-sevir/sub-sevir-train.tar.gz -C sub-sevir/\n",
    "!tar -xf sub-sevir/sub-sevir-val.tar.gz -C sub-sevir/\n",
    "!tar -xf sub-sevir/sub-sevir-test.tar.gz -C sub-sevir/\n",
    "print('grabbing the Github repo')\n",
    "!git clone https://github.com/ai2es/WAF_ML_Tutorial_Part2.git\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Step 1: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#plot parameters that I personally like, feel free to make these your own.\n",
    "import matplotlib\n",
    "import matplotlib.patheffects as path_effects\n",
    "\n",
    "\n",
    "#outlines for text \n",
    "pe1 = [path_effects.withStroke(linewidth=1.5,\n",
    "                             foreground=\"k\")]\n",
    "pe2 = [path_effects.withStroke(linewidth=1.5,\n",
    "                             foreground=\"w\")]\n",
    "\n",
    "matplotlib.rcParams['axes.facecolor'] = [0.9,0.9,0.9] #makes a grey background to the axis face\n",
    "matplotlib.rcParams['axes.labelsize'] = 14 #fontsize in pts\n",
    "matplotlib.rcParams['axes.titlesize'] = 14 \n",
    "matplotlib.rcParams['xtick.labelsize'] = 12 \n",
    "matplotlib.rcParams['ytick.labelsize'] = 12 \n",
    "matplotlib.rcParams['legend.fontsize'] = 12 \n",
    "matplotlib.rcParams['legend.facecolor'] = 'w' \n",
    "matplotlib.rcParams['savefig.transparent'] = False\n",
    "\n",
    "#make default resolution of figures much higher (i.e., High definition)\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Set device to GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Step 2: Load in ```sub-sevir```\n",
    "\n",
    "\n",
    "\n",
    " You'll need to update the following paths to the location where you downloaded the data from Notebook 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = xr.open_dataset('sub-sevir/sub-sevir-train.zarr',engine='zarr')\n",
    "ds_val = xr.open_dataset('sub-sevir/sub-sevir-val.zarr',engine='zarr')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " By design the ```sub-sevir``` data are 'machine learning ready'. This means that the normalization has already been done (i.e., the features have mean 0 and std 1) and the data are already split into the required groups (train/val/test). So in terms of preparing the data, we only have to pick what features we want to use for training and decide what the learning task is (i.e., classification/regression).\n",
    "\n",
    "\n",
    "\n",
    " #### Step 3: Convert data into PyTorch Datasets\n",
    "\n",
    "\n",
    "\n",
    " Before training a machine learning model, we must first convert the data from xarray to PyTorch `Dataset` and `DataLoader` objects. The first task in this tutorial will be a classification (i.e., \"Does this image contain a thunderstorm?\"), so we will choose the ```label_1d_class``` variable as our truth data.\n",
    "\n",
    "\n",
    "\n",
    " **Important Note:** PyTorch expects images in the channel-first format: `(Channels, Height, Width)`, whereas the data comes in `(Height, Width, Channels)`. We will handle this permutation in our custom Dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubSevirDataset(Dataset):\n",
    "    def __init__(self, xarray_dataset):\n",
    "        self.features = xarray_dataset.features.values\n",
    "        self.labels = xarray_dataset.label_1d_class.values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Grab data for index\n",
    "        x = self.features[idx] # Shape: (48, 48, 4)\n",
    "        y = self.labels[idx]   # Shape: (1,) or scalar\n",
    "        \n",
    "        # Convert to torch tensor\n",
    "        x = torch.from_numpy(x).float()\n",
    "        y = torch.tensor(y).float()\n",
    "        \n",
    "        # Permute to (Channels, Height, Width) -> (4, 48, 48)\n",
    "        x = x.permute(2, 0, 1)\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "# Create Datasets\n",
    "train_dataset = SubSevirDataset(ds_train)\n",
    "val_dataset = SubSevirDataset(ds_val)\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Step 4: Reminder of `DataLoader`\n",
    "\n",
    "\n",
    "\n",
    " The concept of a DataLoader is central to PyTorch.\n",
    "\n",
    "\n",
    "\n",
    " To help the machine learning converge on a solution in a timely manner, we first estimate how to adjust the weights of the model (i.e., the gradient of the model with respect to the error) on some *random subset* of the total training data (i.e., batch). This is accomplished by shuffling the data every epoch and randomly selecting batches. PyTorch's `DataLoader` class handles this batching and shuffling for us.\n",
    "\n",
    "\n",
    "\n",
    " Frequently asked question: \"How do I look at the data stored in a `DataLoader`?\"\n",
    "\n",
    "\n",
    "\n",
    " We can *iterate* over this loader. The following code will loop once to get the first batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a single batch\n",
    "data_iter = iter(train_loader)\n",
    "features_batch, labels_batch = next(data_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check shapes\n",
    "print(\"Features shape:\", features_batch.shape) # Should be [32, 4, 48, 48]\n",
    "print(\"Labels shape:\", labels_batch.shape)     # Should be [32]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now you can see that a batch is a collection of 32 images and labels. Note the shape `[32, 4, 48, 48]` which corresponds to `[Batch, Channels, Height, Width]`.\n",
    "\n",
    "\n",
    "\n",
    " We can visualize just one example to show the whole process. We need to permute the dimensions back to `[Height, Width, Channels]` for matplotlib plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the features for example 12 in this batch \n",
    "# Permute back to (H, W, C) for plotting: (1, 2, 0)\n",
    "one_example_features = features_batch[12].permute(1, 2, 0)\n",
    "# Get the label for that same example \n",
    "one_example_label = labels_batch[12]\n",
    "\n",
    "\n",
    "fig,axes = plt.subplots(1,4,figsize=(20,5))\n",
    "axes[0].imshow(one_example_features[:,:,0],cmap='Blues')\n",
    "axes[1].imshow(one_example_features[:,:,1],cmap='turbo')\n",
    "axes[2].imshow(one_example_features[:,:,2],cmap='Spectral_r')\n",
    "axes[3].imshow(one_example_features[:,:,3],cmap='Greys_r')\n",
    "\n",
    "print(f\"Label: {one_example_label.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " there ya go! Now you have a way to dig into the data when they are in this DataLoader form.\n",
    "\n",
    "\n",
    "\n",
    " Just know that if you recreate the iterator or loop again, you will get a new (random) batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Step 5: Build CNN model\n",
    "\n",
    "\n",
    "\n",
    " We will define a Python class inheriting from `nn.Module` to build our CNN.\n",
    "\n",
    "\n",
    "\n",
    " Let's start simple with the following:\n",
    "\n",
    "\n",
    "\n",
    " 1) A single convolutional layer with 3x3 filters and 1 output map.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # Input channels = 4 (from dataset), Output channels = 1 (1 feature map)\n",
    "        # Kernel size = 3\n",
    "        self.conv1 = nn.Conv2d(in_channels=4, out_channels=1, kernel_size=3)\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # We need to calculate the input size for the dense layer.\n",
    "        # Input (48, 48) -> Conv2d(kernel=3, no padding) -> Output (46, 46)\n",
    "        # 46 * 46 * 1 = 2116\n",
    "        self.fc = nn.Linear(46 * 46, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model = SimpleCNN().to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Step 6: Run some data through\n",
    "\n",
    "\n",
    "\n",
    " It can be useful to run your data through the untrained model to see how it performs with completely random weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get predictions\n",
    "def get_predictions(loader, model):\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            preds = model(inputs)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "    return np.array(all_preds).flatten()\n",
    "\n",
    "y_preds = get_predictions(val_loader, model)\n",
    "\n",
    "plt.hist(y_preds)\n",
    "plt.xlabel('prob of lightning')\n",
    "plt.ylabel('count')\n",
    "plt.xlim([0,1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As expected, the model is producing predictions [0,1], and the predictions generally fall in the middle of the probability distribution.\n",
    "\n",
    "\n",
    "\n",
    " Let's take a look at the performance diagram. To do that, we first need to extract the truth labels from our validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_v = ds_val.label_1d_class.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance metrics manually using numpy\n",
    "thresh = np.arange(0.05, 1.05, 0.05)\n",
    "\n",
    "tps = []\n",
    "fps = []\n",
    "fns = []\n",
    "\n",
    "for t in thresh:\n",
    "    # Binarize predictions based on threshold\n",
    "    preds_bin = (y_preds >= t).astype(int)\n",
    "    \n",
    "    # Calculate TP, FP, FN\n",
    "    tp = np.sum((preds_bin == 1) & (y_v == 1))\n",
    "    fp = np.sum((preds_bin == 1) & (y_v == 0))\n",
    "    fn = np.sum((preds_bin == 0) & (y_v == 1))\n",
    "    \n",
    "    tps.append(tp)\n",
    "    fps.append(fp)\n",
    "    fns.append(fn)\n",
    "\n",
    "tps = np.array(tps)\n",
    "fps = np.array(fps)\n",
    "fns = np.array(fns)\n",
    "\n",
    "# Calc x,y of performance diagram \n",
    "# Avoid division by zero\n",
    "pods = np.divide(tps, (tps + fns), out=np.zeros_like(tps, dtype=float), where=(tps + fns)!=0)\n",
    "srs = np.divide(tps, (tps + fps), out=np.zeros_like(tps, dtype=float), where=(tps + fps)!=0)\n",
    "csis = np.divide(tps, (tps + fns + fps), out=np.zeros_like(tps, dtype=float), where=(tps + fns + fps)!=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import some helper functions for our other directory.\n",
    "import sys\n",
    "sys.path.insert(1, 'WAF_ML_Tutorial_Part2/scripts/')\n",
    "\n",
    "#load contingency_table func\n",
    "from gewitter_functions import make_performance_diagram_axis\n",
    "\n",
    "#plot it up  \n",
    "ax = make_performance_diagram_axis()\n",
    "ax.plot(srs, pods, '-s', color='dodgerblue', markerfacecolor='w', label='UNET')\n",
    "\n",
    "for i, t in enumerate(thresh):\n",
    "    text = np.char.ljust(str(np.round(t, 2)), width=4, fillchar='0')\n",
    "    ax.text(srs[i]+0.02, pods[i]+0.02, text, path_effects=pe1, fontsize=9, color='white')\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " There are some funky things to note with this performance diagram. It might seem like random weights work well, but remember that the validation dataset is roughly balanced (no lightning: 44%; lightning: 56%), so even a random guess can demonstrate decent performance. Also notice how the curve appears disjoint.\n",
    "\n",
    "\n",
    "\n",
    " If you want to view the convolution filters, you can access the weights of the layer directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab layer 1 weights\n",
    "filters = model.conv1.weight.data.cpu().numpy()\n",
    "\n",
    "# PyTorch weights are (Out_Channels, In_Channels, H, W) -> (1, 4, 3, 3)\n",
    "print(filters.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's take a look at each of the filters (channels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "# Iterate over the input channels (second dimension)\n",
    "for i in range(4):\n",
    "    # filters[0, i, :, :] gives the filter for the ith input channel\n",
    "    pm = axes[i].imshow(filters[0, i, :, :], vmin=-1, vmax=1, cmap='seismic')\n",
    "    plt.colorbar(pm, ax=axes[i])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This really isn't very informative, but you can see how the values are approximately equally distributed around 0. This distribution is provided in the initialization step when first creating the model.\n",
    "\n",
    "\n",
    "\n",
    " Maybe looking at the output of the convolutional layer will help? We can do this by creating a hook or simply passing data through the first part of the model manually.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model conv layer only\n",
    "# Grab a batch to look at \n",
    "data_iter = iter(val_loader)\n",
    "features, labels = next(data_iter)\n",
    "\n",
    "# Move to device\n",
    "features = features.to(device)\n",
    "\n",
    "# Pass through conv1\n",
    "with torch.no_grad():\n",
    "    convolved_images = model.conv1(features)\n",
    "    \n",
    "convolved_images = convolved_images.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the features for example 0 in this batch \n",
    "one_example_features = features[0].permute(1, 2, 0).cpu() # Back to H,W,C for plotting\n",
    "one_example_label = labels[0]\n",
    "\n",
    "fig,axes = plt.subplots(1,4,figsize=(20,5))\n",
    "axes[0].imshow(one_example_features[:,:,0],cmap='Blues')\n",
    "axes[1].imshow(one_example_features[:,:,1],cmap='turbo')\n",
    "axes[2].imshow(one_example_features[:,:,2],cmap='Spectral_r')\n",
    "axes[3].imshow(one_example_features[:,:,3],cmap='Greys_r')\n",
    "\n",
    "print(f\"Label: {one_example_label}\")\n",
    "\n",
    "plt.figure(figsize=(4.4,5))\n",
    "# Convolved image shape is (Batch, Out_Channels, H, W). \n",
    "# We want example 0, channel 0:\n",
    "plt.imshow(convolved_images[0, 0, :, :])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Notice that even for completely random filter weights (i.e., no training), the CNN still highlights the region where a storm is. This is probably why the performance diagram looks decent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Step 7: Train the model for 20 epochs\n",
    "\n",
    "\n",
    "\n",
    " Okay, enough digging in the weeds. Let's write our training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 20\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train() # Set to train mode\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad() # Zero gradients\n",
    "        outputs = model(inputs) # Forward pass\n",
    "        loss = criterion(outputs.squeeze(), labels) # Calculate loss\n",
    "        loss.backward() # Backprop\n",
    "        optimizer.step() # Update weights\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval() # Set to eval mode\n",
    "    running_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            running_val_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "    epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {epoch_train_loss:.4f} - Val Loss: {epoch_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Depending on your computer, this might take some time to run.\n",
    "\n",
    "\n",
    "\n",
    " #### Step 8: Check for overfitting\n",
    "\n",
    "\n",
    "\n",
    " As always, we should probably check to see if the model is overfitting. To do this, let's plot the loss curves for both the training data and the validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, '-r', label='Training')\n",
    "plt.plot(val_losses, '-b', label='Validation')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Take a moment and consider if overfitting has occurred. Remember, you are looking to see if the red line continues to decrease while the blue line increases.\n",
    "\n",
    "\n",
    "\n",
    " In this example, I would say that the model has probably begun to overfit. Specifically, the validation loss begins to consistently increase around epoch 17. Let's check on the overall performance anyway.\n",
    "\n",
    "\n",
    "\n",
    " #### Step 9: Check performance\n",
    "\n",
    "\n",
    "\n",
    " To check the performance, we can reuse the same code from earlier in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = get_predictions(val_loader, model)\n",
    "\n",
    "plt.hist(y_preds)\n",
    "plt.xlabel('prob of lightning')\n",
    "plt.ylabel('count')\n",
    "plt.xlim([0,1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Here we can see there is a good grouping of examples where the model predicts class probabilities near 0 and 1, and then there are relatively few cases where the output of the model is more in the middle of the distribution (i.e., more uncertain).\n",
    "\n",
    "\n",
    "\n",
    " Let's make the performance diagram again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = np.arange(0.05, 1.05, 0.05)\n",
    "\n",
    "tps = []\n",
    "fps = []\n",
    "fns = []\n",
    "\n",
    "for t in thresh:\n",
    "    preds_bin = (y_preds >= t).astype(int)\n",
    "    tps.append(np.sum((preds_bin == 1) & (y_v == 1)))\n",
    "    fps.append(np.sum((preds_bin == 1) & (y_v == 0)))\n",
    "    fns.append(np.sum((preds_bin == 0) & (y_v == 1)))\n",
    "\n",
    "tps = np.array(tps)\n",
    "fps = np.array(fps)\n",
    "fns = np.array(fns)\n",
    "\n",
    "pods = np.divide(tps, (tps + fns), out=np.zeros_like(tps, dtype=float), where=(tps + fns)!=0)\n",
    "srs = np.divide(tps, (tps + fps), out=np.zeros_like(tps, dtype=float), where=(tps + fps)!=0)\n",
    "csis = np.divide(tps, (tps + fns + fps), out=np.zeros_like(tps, dtype=float), where=(tps + fns + fps)!=0)\n",
    "\n",
    "#plot it up  \n",
    "ax = make_performance_diagram_axis()\n",
    "ax.plot(srs, pods, '-s', color='dodgerblue', markerfacecolor='w', label='UNET')\n",
    "\n",
    "for i, t in enumerate(thresh):\n",
    "    if np.mod(i, 3) == 0:\n",
    "        text = np.char.ljust(str(np.round(t, 2)), width=4, fillchar='0')\n",
    "        ax.text(srs[i]+0.02, pods[i]+0.02, text, path_effects=pe1, fontsize=9, color='white')\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We get much better results this time. The model has a maximum CSI value of about 0.81 when using a class probability of 0.2 as the deterministic threshold.\n",
    "\n",
    "\n",
    "\n",
    " Can we see what filter it learned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab layer 1 weights\n",
    "filters = model.conv1.weight.data.cpu().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "for i in range(4):\n",
    "    pm = axes[i].imshow(filters[0, i, :, :], vmin=-1, vmax=1, cmap='seismic')\n",
    "    plt.colorbar(pm, ax=axes[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As before, it doesn't seem very useful to look at the kernels themselves, so let's look at the output of the convolutional layer instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run just the conv layer\n",
    "data_iter = iter(val_loader)\n",
    "features, labels = next(data_iter)\n",
    "features = features.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    convolved_images = model.conv1(features)\n",
    "convolved_images = convolved_images.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_example_features = features[0].permute(1, 2, 0).cpu() # Back to HWC\n",
    "one_example_label = labels[0]\n",
    "\n",
    "fig,axes = plt.subplots(1,4,figsize=(20,5))\n",
    "axes[0].imshow(one_example_features[:,:,0],cmap='Blues')\n",
    "axes[1].imshow(one_example_features[:,:,1],cmap='turbo')\n",
    "axes[2].imshow(one_example_features[:,:,2],cmap='Spectral_r')\n",
    "axes[3].imshow(one_example_features[:,:,3],cmap='Greys_r')\n",
    "\n",
    "print(f\"Label: {one_example_label}\")\n",
    "\n",
    "plt.figure(figsize=(4.4,5))\n",
    "plt.imshow(convolved_images[0, 0, :, :])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The result here is actually quite similar to the untrained model, although the convolved data does look a bit more 'focused' in on the area where there are cold cloud tops and high radar values.\n",
    "\n",
    "\n",
    "\n",
    " This is probably not a very satisfying explanation for trained models behavior. Thus, later on in this tutorial series there is a notebook discussing more quantitative methods for explaining the CNN.\n",
    "\n",
    "\n",
    "\n",
    " #### Step 10: Save the model\n",
    "\n",
    "\n",
    "\n",
    " Since this model performs pretty well, let's go ahead and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'WAF_ML_Tutorial_Part2/datasets/models/neural_nets_from_notebooks/MyFirstCNN.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Step 11: Add an ANN\n",
    "\n",
    "\n",
    "\n",
    " Typically CNNs have their convolution and maxpool layers, but then after some number of them, the convolutional data is fed through an ANN. This task seems to do well with just a single convolutional layer, but for the sake of completeness, let's add an ANN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ComplexCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=4, out_channels=1, kernel_size=3)\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Conv output is 46x46\n",
    "        self.fc1 = nn.Linear(46 * 46, 16)\n",
    "        self.fc2 = nn.Linear(16, 32)\n",
    "        self.fc3 = nn.Linear(32, 1) # Output layer\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "model = ComplexCNN().to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now you can see there are a few more trainable parameters (i.e., weights).\n",
    "\n",
    "\n",
    "\n",
    " #### Step 12: Train the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 10\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train() \n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    \n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            running_val_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {epoch_train_loss:.4f} - Val Loss: {epoch_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, '-r', label='Training')\n",
    "plt.plot(val_losses, '-b', label='Validation')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This is a clear overfitting signature. It would seem that adding these extra parameters did not help our model.\n",
    "\n",
    "\n",
    "\n",
    " #### Step 13: Add regularization (fix overfitting)\n",
    "\n",
    "\n",
    "\n",
    " This is a good time to introduce some regularization tips that can help avoid overfitting. First, let's try adding dropout. This is where neurons are randomly turned on/off to prevent the neural network from memorizing examples (i.e., a specific pathway for a specific example in your training dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularizedCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RegularizedCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=4, out_channels=1, kernel_size=3)\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.fc1 = nn.Linear(46 * 46, 16)\n",
    "        self.dropout1 = nn.Dropout(0.33) # 33% dropout\n",
    "        self.fc2 = nn.Linear(16, 32)\n",
    "        self.dropout2 = nn.Dropout(0.33) # 33% dropout\n",
    "        self.fc3 = nn.Linear(32, 1) \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x) # Apply dropout\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x) # Apply dropout\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "model = RegularizedCNN().to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 10\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train() \n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    \n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            running_val_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {epoch_train_loss:.4f} - Val Loss: {epoch_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, '-r', label='Training')\n",
    "plt.plot(val_losses, '-b', label='Validation')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As you can see here, the dropout seems to have prevented much of the overfitting signal. However, there may still be some slight overfitting here from about epoch 6 on.\n",
    "\n",
    "\n",
    "\n",
    " #### Step 14: Check performance again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "y_preds = get_predictions(val_loader, model)\n",
    "\n",
    "plt.hist(y_preds)\n",
    "plt.xlabel('prob of lightning')\n",
    "plt.ylabel('count')\n",
    "plt.xlim([0,1])\n",
    "\n",
    "# Calculate performance diagram metrics\n",
    "thresh = np.arange(0.05, 1.05, 0.05)\n",
    "\n",
    "tps = []\n",
    "fps = []\n",
    "fns = []\n",
    "\n",
    "for t in thresh:\n",
    "    preds_bin = (y_preds >= t).astype(int)\n",
    "    tps.append(np.sum((preds_bin == 1) & (y_v == 1)))\n",
    "    fps.append(np.sum((preds_bin == 1) & (y_v == 0)))\n",
    "    fns.append(np.sum((preds_bin == 0) & (y_v == 1)))\n",
    "\n",
    "tps = np.array(tps)\n",
    "fps = np.array(fps)\n",
    "fns = np.array(fns)\n",
    "\n",
    "pods = np.divide(tps, (tps + fns), out=np.zeros_like(tps, dtype=float), where=(tps + fns)!=0)\n",
    "srs = np.divide(tps, (tps + fps), out=np.zeros_like(tps, dtype=float), where=(tps + fps)!=0)\n",
    "csis = np.divide(tps, (tps + fns + fps), out=np.zeros_like(tps, dtype=float), where=(tps + fns + fps)!=0)\n",
    "\n",
    "#plot it up  \n",
    "ax = make_performance_diagram_axis()\n",
    "ax.plot(srs, pods, '-s', color='dodgerblue', markerfacecolor='w', label='UNET')\n",
    "\n",
    "for i, t in enumerate(thresh):\n",
    "    if np.mod(i, 3) == 0:\n",
    "        text = np.char.ljust(str(np.round(t, 2)), width=4, fillchar='0')\n",
    "        ax.text(srs[i]+0.02, pods[i]+0.02, text, path_effects=pe1, fontsize=9, color='white')\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " There ya go! You have trained a more typical CNN structure.\n",
    "\n",
    "\n",
    "\n",
    " The next notebook will dive into using U-nets, which will not only tell us if there is lightning in the image but also where that lightning is."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
