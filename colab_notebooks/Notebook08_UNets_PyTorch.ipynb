{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](http://colab.research.google.com/github/ai2es/WAF_ML_Tutorial_Part2/blob/main/colab_notebooks/Notebook08_UNets.ipynb)\n",
    "\n",
    "\n",
    "\n",
    " # Notebook 08: U-Networks\n",
    "\n",
    "\n",
    "\n",
    " ### Primary Goal:\n",
    "\n",
    "\n",
    "\n",
    " Build and train a \"U\"-Network\n",
    "\n",
    "\n",
    "\n",
    " #### Background\n",
    "\n",
    "\n",
    "\n",
    " As an added step of complexity, the next version of neural networks covered in the paper are called \"U\"-networks. U-Nets are still convolutional neural networks, but now they have a 'down' and an 'up' branch which gives the network architecture the titular \"U\" shape. The down (or encoding) branch of the network is used to convolve and maxpool the input data, reducing the image size. Then on the 'up' (or decoding) branch, the image is upsampled back to a higher resolution (typically matching the input's original resolution). Figure 6 of the paper is reproduced here for additional context.\n",
    "\n",
    "\n",
    "\n",
    " <img src=\"../images/Fig6_WAF2.png\" width=\"800\" height=\"200\" class=\"center\" />\n",
    "\n",
    "\n",
    "\n",
    " So what does a U-Net do that a CNN can't?\n",
    "\n",
    "\n",
    "\n",
    " - The main strength of U-Nets is 'image-to-image' translation. In other words the U-Net takes in some input image and then outputs an image with the same shape as the input. For our example, we can use a U-Net to not only predict if there is lightning in the image, but also where in the image it is located.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " #### Step 0: Installs and grab the data\n",
    "\n",
    "\n",
    "\n",
    " Note: downloading the data can take up to 15-20 mins depending on the server speed. Please be patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Install and unpack files [this will take some time]\n",
    "\n",
    "print('installing dependencies')\n",
    "!pip -q install zenodo-get\n",
    "!pip -q install zarr \n",
    "# Note: We are removing the keras-unet-collection install as we will implement UNet in PyTorch manually\n",
    "print('grabbing sub-sevir')\n",
    "!zenodo_get 7011372\n",
    "print('unpacking sub-sevir')\n",
    "!tar -xf sub-sevir.tar.gz\n",
    "!tar -xf sub-sevir/sub-sevir-train.tar.gz -C sub-sevir/\n",
    "!tar -xf sub-sevir/sub-sevir-val.tar.gz -C sub-sevir/\n",
    "!tar -xf sub-sevir/sub-sevir-test.tar.gz -C sub-sevir/\n",
    "print('grabbing the Github repo')\n",
    "!git clone https://github.com/ai2es/WAF_ML_Tutorial_Part2.git\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Step 1: Imports\n",
    "\n",
    "\n",
    "\n",
    " As with all of our notebooks, we will first import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "#plot parameters that I personally like, feel free to make these your own.\n",
    "import matplotlib\n",
    "import matplotlib.patheffects as path_effects\n",
    "\n",
    "\n",
    "#outlines for text \n",
    "pe1 = [path_effects.withStroke(linewidth=1.5,\n",
    "                             foreground=\"k\")]\n",
    "pe2 = [path_effects.withStroke(linewidth=1.5,\n",
    "                             foreground=\"w\")]\n",
    "\n",
    "matplotlib.rcParams['axes.facecolor'] = [0.9,0.9,0.9] #makes a grey background to the axis face\n",
    "matplotlib.rcParams['axes.labelsize'] = 14 #fontsize in pts\n",
    "matplotlib.rcParams['axes.titlesize'] = 14 \n",
    "matplotlib.rcParams['xtick.labelsize'] = 12 \n",
    "matplotlib.rcParams['ytick.labelsize'] = 12 \n",
    "matplotlib.rcParams['legend.fontsize'] = 12 \n",
    "matplotlib.rcParams['legend.facecolor'] = 'w' \n",
    "matplotlib.rcParams['savefig.transparent'] = False\n",
    "\n",
    "#make default resolution of figures much higher (i.e., High definition)\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Step 2: Define the U-Net Model\n",
    "\n",
    "\n",
    "\n",
    " Since we are using PyTorch, we will implement the U-Net architecture manually rather than relying on a Keras wrapper. This gives us more control and is standard practice in PyTorch.\n",
    "\n",
    "\n",
    "\n",
    " A U-Net consists of an encoder (downsampling path) and a decoder (upsampling path) with skip connections.\n",
    "\n",
    "\n",
    "\n",
    " We will define `DoubleConv`, `Down`, `Up`, and `OutConv` blocks to build the full `UNet` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        \n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "    \n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=False):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        # Standard UNet structure, but reduced depth for this tutorial similar to the original keras notebook\n",
    "        # Input channels: 4 (features)\n",
    "        # We start with 32 filters (can be adjusted)\n",
    "        self.inc = DoubleConv(n_channels, 32)\n",
    "        self.down1 = Down(32, 64)\n",
    "        self.down2 = Down(64, 128)\n",
    "        self.down3 = Down(128, 256)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(256, 512 // factor)\n",
    "        \n",
    "        self.up1 = Up(512, 256 // factor, bilinear)\n",
    "        self.up2 = Up(256, 128 // factor, bilinear)\n",
    "        self.up3 = Up(128, 64 // factor, bilinear)\n",
    "        self.up4 = Up(64, 32, bilinear)\n",
    "        self.outc = OutConv(32, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return torch.sigmoid(logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Step 3: Load in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = xr.open_dataset('sub-sevir/sub-sevir-train.zarr',engine='zarr')\n",
    "ds_val = xr.open_dataset('sub-sevir/sub-sevir-val.zarr',engine='zarr')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Next, let's convert the data to PyTorch `Dataset` and `DataLoader`.\n",
    "\n",
    "\n",
    "\n",
    " #### Step 4: Make PyTorch Dataset and DataLoader\n",
    "\n",
    "\n",
    "\n",
    " This will be very similar to the process used in the previous notebook.  The only difference is this time we will use the ```2d``` labels instead of the ```1d``` labels.\n",
    "\n",
    "\n",
    "\n",
    " **Important Note on Shapes:** PyTorch expects images in `(Channel, Height, Width)` format. Our input data is likely `(Height, Width, Channel)`. We need to permute dimensions in our Dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubSevirDataset(Dataset):\n",
    "    def __init__(self, xarray_dataset):\n",
    "        self.features = xarray_dataset.features.values\n",
    "        self.labels = xarray_dataset.label_2d_class.values # Using 2d labels now!\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Grab data\n",
    "        x = self.features[idx] # (H, W, C)\n",
    "        y = self.labels[idx]   # (H, W) or (H, W, 1)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        x = torch.from_numpy(x).float()\n",
    "        y = torch.from_numpy(y).float()\n",
    "        \n",
    "        # Permute Input: (H, W, C) -> (C, H, W)\n",
    "        x = x.permute(2, 0, 1)\n",
    "        \n",
    "        # Ensure label has channel dim: (H, W) -> (1, H, W)\n",
    "        if y.ndim == 2:\n",
    "            y = y.unsqueeze(0)\n",
    "        elif y.ndim == 3 and y.shape[-1] == 1:\n",
    "            y = y.permute(2, 0, 1)\n",
    "            \n",
    "        return x, y\n",
    "\n",
    "# Instantiate Datasets\n",
    "train_dataset = SubSevirDataset(ds_train)\n",
    "val_dataset = SubSevirDataset(ds_val)\n",
    "\n",
    "# Set seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create Loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Next, let's look at an example from the dataset to verify everything worked as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one batch\n",
    "data_iter = iter(train_loader)\n",
    "features_batch, labels_batch = next(data_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the features for example 29 in this batch \n",
    "# Permute back to HWC for plotting\n",
    "one_example_features = features_batch[29].permute(1, 2, 0)\n",
    "# Get the label for that same example \n",
    "one_example_label = labels_batch[29].squeeze() # Remove channel dim for plotting\n",
    "\n",
    "\n",
    "fig,axes = plt.subplots(1,4,figsize=(20,5))\n",
    "axes[0].imshow(one_example_features[:,:,0],cmap='Blues')\n",
    "axes[1].imshow(one_example_features[:,:,1],cmap='turbo')\n",
    "axes[2].imshow(one_example_features[:,:,2],cmap='Spectral_r')\n",
    "axes[3].imshow(one_example_features[:,:,3],cmap='Greys_r')\n",
    "\n",
    "fig,axes = plt.subplots(1,1,figsize=(4,5))\n",
    "axes.imshow(one_example_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The first row of images is still our features (i.e., inputs), which are Water Vapor, Infrared, Radar and Visible. The second row is the label containing a map of the lightning locations. The label is 0 (blue) where there is no lightning and 1 (yellow) where there is lightning. The goal for the U-Net is to produce a map containing the probability of lightning at each pixel.\n",
    "\n",
    "\n",
    "\n",
    " #### Step 5: Build the model\n",
    "\n",
    "\n",
    "\n",
    " We initialize the `UNet` class we defined earlier.\n",
    "\n",
    "\n",
    "\n",
    " * `n_channels`: 4 (Our 4 weather features)\n",
    "\n",
    " * `n_classes`: 1 (Probability map of lightning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = UNet(n_channels=4, n_classes=1).to(device)\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Step 6: Run some data through\n",
    "\n",
    "\n",
    "\n",
    " Okay, let's run some data through the untrained model just to see what a randomly weighted model looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for predictions\n",
    "def get_predictions(loader, model):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            preds = model(inputs)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "    return np.concatenate(all_preds)\n",
    "\n",
    "y_preds = get_predictions(val_loader, model)\n",
    "\n",
    "plt.hist(y_preds.ravel())\n",
    "plt.xlabel('prob of lightning')\n",
    "plt.ylabel('count')\n",
    "plt.xlim([0,1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The results are a bit different this time. It looks like most of the predictions are near 0.5, which could be expected for random weights. Let's look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the features for example 27 in this batch (from the validation loader iteration)\n",
    "# We need to manually iterate the val_loader to get matching batch data for visualization\n",
    "val_iter = iter(val_loader)\n",
    "val_features_batch, val_labels_batch = next(val_iter)\n",
    "\n",
    "one_example_features = val_features_batch[27].permute(1, 2, 0)\n",
    "one_example_label = val_labels_batch[27].squeeze()\n",
    "one_example_pred = y_preds[27].squeeze() # Remove channel dim (1, 48, 48) -> (48, 48)\n",
    "\n",
    "fig,axes = plt.subplots(1,4,figsize=(20,5))\n",
    "axes[0].imshow(one_example_features[:,:,0],cmap='Blues')\n",
    "axes[1].imshow(one_example_features[:,:,1],cmap='turbo')\n",
    "axes[2].imshow(one_example_features[:,:,2],cmap='Spectral_r')\n",
    "axes[3].imshow(one_example_features[:,:,3],cmap='Greys_r')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "fig,axes = plt.subplots(1,2,figsize=(10,5))\n",
    "pm = axes[0].imshow(one_example_label)\n",
    "plt.colorbar(pm,ax=axes[0])\n",
    "pm = axes[1].imshow(one_example_pred)\n",
    "plt.colorbar(pm,ax=axes[1],label='lightning_prob')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As expected, the untrained model appears to be randomly guessing where the lightning is in the image.\n",
    "\n",
    "\n",
    "\n",
    " #### Step 7: Compile and train\n",
    "\n",
    "\n",
    "\n",
    " We define our optimizer (Adam) and loss function (Binary Cross Entropy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "epochs = 5\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "    epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_val_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "    epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {epoch_train_loss:.4f} - Val Loss: {epoch_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This may take some time to run depending on your computer. On my particular Macbook Pro, the training takes ~ 1 min per epoch. However, if you are running this on Google Colab, the training should run much faster if you have the GPU enabled. This is where the power of GPUs can be really useful for machine learning. You can run many more training sessions when the code takes ~ 10s per epoch instead of 1 min.\n",
    "\n",
    "\n",
    "\n",
    " #### Step 8: Run data through 'trained' model\n",
    "\n",
    "\n",
    "\n",
    " Like before, let's run some data through our newly trained network\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = get_predictions(val_loader, model)\n",
    "\n",
    "plt.hist(y_preds.ravel())\n",
    "plt.xlabel('prob of lightning')\n",
    "plt.ylabel('count')\n",
    "plt.xlim([0,1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now most of the output is 0. This is not surprising as lightning is a rare event within the pixels of each image in the dataset.  That is, lightning is typically observed in only a few pixels in each example. Let's see if things make sense spatially by plotting the same example we showed before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the 27th example of the first batch again\n",
    "# Note: y_preds corresponds to the entire validation set, so index 27 is the 27th item of the dataset.\n",
    "# We need to fetch the 27th item from the dataset directly to match.\n",
    "val_features, val_label = val_dataset[27]\n",
    "\n",
    "one_example_features = val_features.permute(1, 2, 0)\n",
    "one_example_label = val_label.squeeze()\n",
    "one_example_pred = y_preds[27].squeeze()\n",
    "\n",
    "fig,axes = plt.subplots(1,4,figsize=(20,5))\n",
    "axes[0].imshow(one_example_features[:,:,0],cmap='Blues')\n",
    "axes[1].imshow(one_example_features[:,:,1],cmap='turbo')\n",
    "axes[2].imshow(one_example_features[:,:,2],cmap='Spectral_r')\n",
    "axes[3].imshow(one_example_features[:,:,3],cmap='Greys_r')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "fig,axes = plt.subplots(1,2,figsize=(10,5),facecolor='w')\n",
    "pm = axes[0].imshow(one_example_label)\n",
    "plt.colorbar(pm,ax=axes[0])\n",
    "pm = axes[1].imshow(one_example_pred)\n",
    "plt.colorbar(pm,ax=axes[1],label='lightning_prob')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Good! The model is at least getting the general area of lighting correct. Notice though that the probabilities are quite low (in an absolute sense). Now let's take a look at the performance diagram which should help us better assess this rare event. This will be a pixel-by-pixel analysis, so there are A LOT more samples than  in previous notebooks.\n",
    "\n",
    "\n",
    "\n",
    " #### Step 8: Evaluate\n",
    "\n",
    "\n",
    "\n",
    " First we have to get the truth values for each pixel. We are going to ravel these into a 1-d vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all labels from the dataset\n",
    "all_labels = []\n",
    "for _, label in val_loader:\n",
    "    all_labels.append(label.numpy())\n",
    "y_v = np.concatenate(all_labels)\n",
    "\n",
    "#ravel both\n",
    "y_v_rav = y_v.ravel()\n",
    "y_preds_rav = y_preds.ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate stats manually for performance diagram\n",
    "thresh = np.arange(0.05, 1.05, 0.05)\n",
    "\n",
    "tps = []\n",
    "fps = []\n",
    "fns = []\n",
    "\n",
    "# This might be slow for pixel-wise calculation on the full array\n",
    "# Ideally, use GPU or optimized metrics if available\n",
    "for t in thresh:\n",
    "    preds_bin = (y_preds_rav >= t).astype(int)\n",
    "    truth_bin = (y_v_rav == 1).astype(int)\n",
    "    \n",
    "    tp = np.sum((preds_bin == 1) & (truth_bin == 1))\n",
    "    fp = np.sum((preds_bin == 1) & (truth_bin == 0))\n",
    "    fn = np.sum((preds_bin == 0) & (truth_bin == 1))\n",
    "    \n",
    "    tps.append(tp)\n",
    "    fps.append(fp)\n",
    "    fns.append(fn)\n",
    "\n",
    "tps = np.array(tps)\n",
    "fps = np.array(fps)\n",
    "fns = np.array(fns)\n",
    "\n",
    "# Avoid div by zero\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "    pods = tps/(tps + fns)\n",
    "    srs = tps/(tps + fps)\n",
    "    csis = tps/(tps + fns + fps)\n",
    "    \n",
    "pods = np.nan_to_num(pods)\n",
    "srs = np.nan_to_num(srs)\n",
    "csis = np.nan_to_num(csis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import some helper functions for our other directory.\n",
    "import sys\n",
    "sys.path.insert(1, 'WAF_ML_Tutorial_Part2/scripts/')\n",
    "\n",
    "#load contingency_table func\n",
    "from gewitter_functions import make_performance_diagram_axis\n",
    "\n",
    "#plot it up  \n",
    "ax = make_performance_diagram_axis()\n",
    "ax.plot(srs, pods,'-s',color='dodgerblue',markerfacecolor='w',label='UNET')\n",
    "\n",
    "for i,t in enumerate(thresh):\n",
    "    text = np.char.ljust(str(np.round(t,2)),width=4,fillchar='0')\n",
    "    ax.text(srs[i]+0.02, pods[i]+0.02,text,path_effects=pe1,fontsize=9,color='white')\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Overall, the performance is not that great. However, we only trained the U-Net for a limited number of epochs, so the network may not have fully converged on a solution. Also remember that this is a much more difficult classification problem than that demonstrated in the other notebooks (i.e., the ANN and CNN ones). We not only want to know if there is lightning in the image, but also *where* that lightning is. Therefore, we may need to calibrate our expectations.\n",
    "\n",
    "\n",
    "\n",
    " #### Step 8: Try a slightly different batch_size\n",
    "\n",
    "\n",
    "\n",
    " You wouldn't be able to tell at this point, but it took us a decent amount of time to find a U-Net that 'works' when trained on a CPU in a reasonable amount of time. To demonstrate the challenge, let's lower the training batch size to 32 instead of 64.\n",
    "\n",
    "\n",
    "\n",
    " We need to re-load the datasets and re-batch them as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new Loaders\n",
    "batch_size = 32\n",
    "train_loader_32 = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader_32 = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize model\n",
    "model_32 = UNet(n_channels=4, n_classes=1).to(device)\n",
    "optimizer = optim.Adam(model_32.parameters(), lr=1e-3)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Train\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model_32.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader_32:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_32(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    print(f\"Epoch {epoch+1} Loss: {running_loss / len(train_loader_32.dataset):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = get_predictions(val_loader_32, model_32)\n",
    "\n",
    "plt.hist(y_preds.ravel())\n",
    "plt.xlabel('prob of lightning')\n",
    "plt.ylabel('count')\n",
    "# plt.xlim([0,1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As you can see here, the lower batch size results in some *oddities*. It looks like the model is predicting one value basically everywhere. But this becomes even more clear when we look at the same example again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get example 27 from the new loader\n",
    "val_features, val_label = val_dataset[27]\n",
    "\n",
    "one_example_features = val_features.permute(1, 2, 0)\n",
    "one_example_label = val_label.squeeze()\n",
    "one_example_pred = y_preds[27].squeeze()\n",
    "\n",
    "fig,axes = plt.subplots(1,4,figsize=(20,5))\n",
    "axes[0].imshow(one_example_features[:,:,0],cmap='Blues')\n",
    "axes[1].imshow(one_example_features[:,:,1],cmap='turbo')\n",
    "axes[2].imshow(one_example_features[:,:,2],cmap='Spectral_r')\n",
    "axes[3].imshow(one_example_features[:,:,3],cmap='Greys_r')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "fig,axes = plt.subplots(1,2,figsize=(10,5),facecolor='w')\n",
    "pm = axes[0].imshow(one_example_label)\n",
    "plt.colorbar(pm,ax=axes[0])\n",
    "pm = axes[1].imshow(one_example_pred)\n",
    "plt.colorbar(pm,ax=axes[1],label='lightning_prob')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As we thought, the model is essentially predicting the same value for every pixel in the image. Why does this happen? Perhaps because lightning is a rare event, the model learned to just never predict it while still achieving decent scores in the context of the loss function. So how does increasing the batch size to 64 help fix this? One theory is that the larger batch size contains more examples of lightning, which then have a greater influence on the loss function. Furthermore, a batch size of 64 will provide a better estimate of the *true* gradient, which should lead to better weights and an improved loss function score.\n",
    "\n",
    "\n",
    "\n",
    " This is probably more discussion than necessary for this tutorial, but we wanted to demonstrate that training these ML models is not always as straightforward as it appears. It can be very frustrating at times.\n",
    "\n",
    "\n",
    "\n",
    " #### Step 9: Load the U-Net from the paper\n",
    "\n",
    "\n",
    "\n",
    " *Note: Loading a Keras-trained H5 model directly into PyTorch is not straightforward because the architectures and weight definitions must match exactly. For this tutorial, we will skip the loading of the external Keras model and rely on the PyTorch model we just trained.*\n",
    "\n",
    "\n",
    "\n",
    " The next notebook will dive into methods that can be used to help explain the performance of CNNs."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
