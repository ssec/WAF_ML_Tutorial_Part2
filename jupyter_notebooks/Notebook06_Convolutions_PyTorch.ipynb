{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Notebook 06: Convolutions\n",
    "\n",
    "\n",
    "\n",
    " ### Primary Goal:\n",
    "\n",
    "\n",
    "\n",
    " Explore convolutions and how they work\n",
    "\n",
    "\n",
    "\n",
    " #### Background\n",
    "\n",
    "\n",
    "\n",
    " Convolutional Neural Networks (CNNs) are powerful machine learning models that can learn both large- and small-scale patterns from multi-dimensional data.  Before we can train a CNN, it is necessary to understand what convolutions are and how they work. Convolutions can be a bit confusing if you are unfamiliar with them, so let's take the time here to explore these important machine learning techniques.\n",
    "\n",
    "\n",
    "\n",
    " #### Step 1: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import tqdm \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.patheffects as path_effects\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "#outlines for text \n",
    "pe1 = [path_effects.withStroke(linewidth=1.5,\n",
    "                             foreground=\"k\")]\n",
    "pe2 = [path_effects.withStroke(linewidth=1.5,\n",
    "                             foreground=\"w\")]\n",
    "\n",
    "\n",
    "#plot parameters that I personally like, feel free to make these your own.\n",
    "matplotlib.rcParams['axes.facecolor'] = [0.9,0.9,0.9]\n",
    "matplotlib.rcParams['axes.labelsize'] = 14\n",
    "matplotlib.rcParams['axes.titlesize'] = 14\n",
    "matplotlib.rcParams['xtick.labelsize'] = 12\n",
    "matplotlib.rcParams['ytick.labelsize'] = 12\n",
    "matplotlib.rcParams['legend.fontsize'] = 12\n",
    "matplotlib.rcParams['legend.facecolor'] = 'w'\n",
    "matplotlib.rcParams['savefig.transparent'] = False\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "#one quick thing here, we need to set the random seed so we all get the same results no matter the computer or python session \n",
    "torch.manual_seed(43)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Step 2: Load in some data\n",
    "\n",
    "\n",
    "\n",
    " For this notebook we will use the same hook echo example shown in the paper (e.g., Figures 3 and 4).  The data for this example comes from [Lagerquist et al. (2020)](https://journals.ametsoc.org/view/journals/mwre/148/7/mwrD190372.xml). For convenience, we have already isolated the one storm that has a prominent hook echo on radar reflectivity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_sample = xr.open_dataset('../datasets/lagerquist_2020/lagerquist_storm_example.nc',engine='netcdf4')\n",
    "\n",
    "#print out the dataset \n",
    "ds_sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Step 3: Reminder about what images are\n",
    "\n",
    "\n",
    "\n",
    " Remember that images are just matrices of values, where the values determine the color that is shown in the image. To drive this point home, let's zoom into the IR image from Notebook 2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import some helper functions for our other directory.\n",
    "import sys\n",
    "sys.path.insert(1, '../scripts/')\n",
    "\n",
    "#load contingency_table func\n",
    "from aux_functions import show_vals\n",
    "\n",
    "\n",
    "#make a big figure so we can see the pixels\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(ds_sample.radar_image_matrix[:,:,0,0],vmin=0,vmax=60,cmap='Spectral_r')\n",
    "plt.gca().axis('off')\n",
    "\n",
    "#draw box to see where the next plot will zoom into\n",
    "x_vertices = np.array([4.,16.,16.,4.,4.,])\n",
    "y_vertices = np.array([10.,10.,22.,22.,10.])\n",
    "plt.plot(x_vertices-0.5,y_vertices-0.5,'-w',lw=2)\n",
    "\n",
    "# #add manual annotation \n",
    "plt.text(2,6,'Zoom in box',color='w',fontsize=32)\n",
    "\n",
    "\n",
    "#make a big figure so we can see the pixels\n",
    "plt.figure(figsize=(15,15))\n",
    "da = ds_sample.radar_image_matrix.isel(grid_row=slice(10,22),grid_column=slice(4,16),radar_height=0,radar_field=0)\n",
    "plt.imshow(da,vmin=0,vmax=60,cmap='Spectral_r')\n",
    "show_vals(da,plt.gca())\n",
    "plt.gca().axis('off')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Hopefully this helps illustrate that images are just matrices of values.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " Next, we will take a step-by-step look at the convolution process to demonstrate what a convolution does and how it is used in CNNs.\n",
    "\n",
    "\n",
    "\n",
    " #### Step 4: What are Convolutions?\n",
    "\n",
    "\n",
    "\n",
    " I often find animations can be helpful for describing an idea. Below is an animated version of Figure 3 from the paper. Don't get bogged down in the math right now, just notice how the kernel (filter) is incrementally stepped through the image systematically. This example is the same 12x12 image (the zoom in) from above.\n",
    "\n",
    "\n",
    "\n",
    " <center><img src=\"../images/convolution_animation_01.gif\" alt=\"drawing\" width=\"600\"/></center>\n",
    "\n",
    "\n",
    "\n",
    " Hopefully it is more apparent now what a ```convolution``` is. At this point we have not shown any machine learning - this is simply an image processing technique (i.e., like a mathematical operator). For those more mathematically inclined, the convolution is:\n",
    "\n",
    "\n",
    "\n",
    " $$\\begin{equation} p_{x,y}= \\sum_{j=y-k}^{j=y+k}\\sum_{i=x-k}^{i=x+k} w_{i,j} p_{i,j}, \\label{e3} \\end{equation}$$\n",
    "\n",
    "\n",
    "\n",
    " where $p_{x,y}$ is the pixel value at position x,y in the image matrix and k is there kernel size. This equation is then iterated for all pixels in the image.\n",
    "\n",
    "\n",
    "\n",
    " Normally, the convolution kernel (the middle picture in the animation) has weights that are more sophisticated than what was shown. For example, you might have noticed the right image is the same as the left image, just with a white border around it. That is because the example uses the identity kernel to set the convolution weights.\n",
    "\n",
    "\n",
    "\n",
    " $$\\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}$$\n",
    "\n",
    "\n",
    "\n",
    " Thus, the kernel is simply passing the middle pixel through with no change. Other common kernels in image processing are:\n",
    "\n",
    "\n",
    "\n",
    " ##### **Sharpen:** #\n",
    "\n",
    " $$\\begin{bmatrix} 0 & -1 & 0 \\\\ -1 & 5 & -1 \\\\ 0 & -1 & 0 \\end{bmatrix}$$\n",
    "\n",
    "\n",
    "\n",
    " Here is this kernel being applied to our zoomed-in patch:\n",
    "\n",
    "\n",
    "\n",
    " <center><img src=\"../images/sharpen_image.png\" alt=\"drawing\" width=\"600\"/></center>\n",
    "\n",
    "\n",
    "\n",
    " and on the entire image:\n",
    "\n",
    "\n",
    "\n",
    " <center><img src=\"../images/sharpen_image_full.png\" alt=\"drawing\" width=\"600\"/></center>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " ##### **Blur** (i.e., mean)\n",
    "\n",
    "\n",
    "\n",
    " $$ \\frac{1}{9} \\begin{bmatrix} 1 & 1 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 1 & 1 \\end{bmatrix}$$\n",
    "\n",
    "\n",
    "\n",
    " Likewise, here is the blur kernel applied to the zoomed-in patch\n",
    "\n",
    "\n",
    "\n",
    " <center><img src=\"../images/blur_image.png\" alt=\"drawing\" width=\"600\"/></center>\n",
    "\n",
    "\n",
    "\n",
    " and on the whole image:\n",
    "\n",
    "\n",
    "\n",
    " <center><img src=\"../images/blur_image_full.png\" alt=\"drawing\" width=\"600\"/></center>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " I would recommend checking out the webpage [here](https://en.wikipedia.org/wiki/Kernel_(image_processing)) for more examples.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " Again, we have not actually applied any machine learning so far. The discussion has solely been focused on the convolution operation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " #### Step 5: Implement a convolution with code\n",
    "\n",
    "\n",
    "\n",
    " It is possible to manually code a convolution yourself, but for the sake of simplicity and speed, PyTorch has a convolutional layer that handles this for you:\n",
    "\n",
    "\n",
    "\n",
    " ```torch.nn.Conv2d()```\n",
    "\n",
    "\n",
    "\n",
    " We will need to configure the layer to use the desired set of weights, but this is relatively simple to do.\n",
    "\n",
    "\n",
    "\n",
    " **Important Note on Data Shapes:** PyTorch expects images in the format `(Batch, Channels, Height, Width)`, often abbreviated as NCHW. However, many image libraries (and Matplotlib) use `(Height, Width, Channels)`. We will need to reshape our data accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert xarray data to numpy, then to a torch tensor\n",
    "# We want to resize it to 36x36. \n",
    "input_data = ds_sample.radar_image_matrix.isel(radar_height=slice(0,1),radar_field=0).values\n",
    "\n",
    "# Convert to torch tensor and add batch/channel dimensions to match PyTorch expectations: (Batch, Channel, Height, Width)\n",
    "# Original is (H, W), we need (1, 1, H, W)\n",
    "input_tensor = torch.from_numpy(input_data).unsqueeze(0).unsqueeze(0).float()\n",
    "\n",
    "# Resize using interpolate.\n",
    "more_points = F.interpolate(input_tensor, size=(36, 36), mode='bilinear', align_corners=False)\n",
    "\n",
    "#define the sharpen filter\n",
    "kernel_weights = np.array([[ 0, -1, 0],[ -1,5,-1],[0,-1,0]])\n",
    "\n",
    "# Define conv with specific weights. \n",
    "# PyTorch Conv2d weights shape is (Out_Channels, In_Channels, Kernel_H, Kernel_W)\n",
    "conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, bias=False)\n",
    "\n",
    "# Set the weights manually\n",
    "with torch.no_grad():\n",
    "    # We reshape our 3x3 kernel to (1, 1, 3, 3)\n",
    "    conv.weight.data = torch.from_numpy(kernel_weights).float().view(1, 1, 3, 3)\n",
    "\n",
    "# Run the data through \n",
    "res = conv(more_points)\n",
    "\n",
    "\n",
    "# Plotting\n",
    "fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\n",
    "\n",
    "# We need to squeeze the dimensions back out to (36, 36) for matplotlib\n",
    "ax1.imshow(more_points.squeeze(), vmin=0, vmax=60, cmap='Spectral_r')\n",
    "ax1.set_title('Original')\n",
    "ax1.axis('off')\n",
    "\n",
    "# Show result. \n",
    "ax2.imshow(res.detach().squeeze(), vmin=0, vmax=60, cmap='Spectral_r')\n",
    "ax2.set_title('Convolved')\n",
    "ax2.axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As you hopefully can see, PyTorch's built-in convolution layer is a convenient and easy way to perform a convolution.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " #### Step 6: Try a new image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define URL of image here. (right click an image online and get image address)\n",
    "url = '[https://dopplerchase-ai2es-schooner-hpc.readthedocs.io/en/latest/_images/ai2es-logo-web-trans.png](https://dopplerchase-ai2es-schooner-hpc.readthedocs.io/en/latest/_images/ai2es-logo-web-trans.png)'\n",
    "\n",
    "#load in some image packages, dont worry about these\n",
    "from PIL import Image,ImageOps\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "#this grabs the image and turns it into an array of data we can use\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "img = ImageOps.grayscale(img)\n",
    "arr = np.array(img.convert('F'))\n",
    "\n",
    "# Preprocess for PyTorch: (H, W) -> (1, 1, H, W)\n",
    "arr_tensor = torch.from_numpy(arr).unsqueeze(0).unsqueeze(0).float()\n",
    "\n",
    "# Resize the image so this code works for any image given\n",
    "# PyTorch interpolate expects (Batch, Channel, H, W)\n",
    "arr_tensor = F.interpolate(arr_tensor, size=(260, 260), mode='bilinear', align_corners=False)\n",
    "\n",
    "\n",
    "#define a filter!\n",
    "\n",
    "#blur \n",
    "kernel_weights = np.array([[ 1, 1, 1],[ 1,1,1],[1,1,1]])*(1/9.)\n",
    "\n",
    "#sharpen\n",
    "# kernel_weights = np.array([[ 0, -1, 0],[ -1,5,-1],[0,-1,0]])\n",
    "\n",
    "# Define conv with specific weights\n",
    "conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, bias=False)\n",
    "\n",
    "# Set weights\n",
    "with torch.no_grad():\n",
    "    conv.weight.data = torch.from_numpy(kernel_weights).float().view(1, 1, 3, 3)\n",
    "\n",
    "# Run the conv\n",
    "res = conv(arr_tensor)\n",
    "\n",
    "\n",
    "# Plot it up. \n",
    "fig,(ax1,ax2,ax3) = plt.subplots(1,3,figsize=(15,5),facecolor='w')\n",
    "\n",
    "# Squeeze dimensions for plotting (1, 1, H, W) -> (H, W)\n",
    "input_img = arr_tensor.squeeze()\n",
    "output_img = res.detach().squeeze()\n",
    "\n",
    "ax1.imshow(input_img, vmin=0, vmax=255, cmap='Greys_r')\n",
    "ax1.set_title('Original')\n",
    "ax1.axis('off')\n",
    "\n",
    "# Show result. \n",
    "ax2.imshow(output_img, vmin=0, vmax=255, cmap='Greys_r')\n",
    "ax2.set_title('Convolved')\n",
    "ax2.axis('off')\n",
    "\n",
    "# Calculate difference. We need to slice the input to match the output size (convolution reduces size by kernel_size - 1)\n",
    "# PyTorch and TF handle this similarly (valid padding), losing 1 pixel on each side for a 3x3 kernel.\n",
    "diff = input_img[1:-1, 1:-1] - output_img\n",
    "\n",
    "ax3.imshow(diff, vmin=-100, vmax=100, cmap='seismic')\n",
    "ax3.set_title('Difference')\n",
    "ax3.axis('off')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " How about a space cowboy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = '[https://cdn.openai.com/dall-e-2/demos/text2im/astronaut/horse/photo/0.jpg](https://cdn.openai.com/dall-e-2/demos/text2im/astronaut/horse/photo/0.jpg)'\n",
    "\n",
    "#load in some image packages, dont worry about these\n",
    "from PIL import Image,ImageOps\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "#this grabs the image and turns it into an array of data we can use\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "img = ImageOps.grayscale(img)\n",
    "arr = np.array(img.convert('F'))\n",
    "\n",
    "# Preprocess for PyTorch: (H, W) -> (1, 1, H, W)\n",
    "arr_tensor = torch.from_numpy(arr).unsqueeze(0).unsqueeze(0).float()\n",
    "\n",
    "# Resize the image so this code works for any image given\n",
    "arr_tensor = F.interpolate(arr_tensor, size=(260, 260), mode='bilinear', align_corners=False)\n",
    "\n",
    "\n",
    "#define a filter!\n",
    "\n",
    "#blur \n",
    "kernel_weights = np.array([[ 1, 1, 1],[ 1,1,1],[1,1,1]])*(1/9.)\n",
    "\n",
    "#sharpen\n",
    "# kernel_weights = np.array([[ 0, -1, 0],[ -1,5,-1],[0,-1,0]])\n",
    "\n",
    "# Define conv with specific weights\n",
    "conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, bias=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    conv.weight.data = torch.from_numpy(kernel_weights).float().view(1, 1, 3, 3)\n",
    "\n",
    "#run the conv\n",
    "res = conv(arr_tensor)\n",
    "\n",
    "\n",
    "#plot it up. a figure with 3 subplots in the column direction \n",
    "fig,(ax1,ax2,ax3) = plt.subplots(1,3,figsize=(15,5))\n",
    "\n",
    "input_img = arr_tensor.squeeze()\n",
    "output_img = res.detach().squeeze()\n",
    "\n",
    "ax1.imshow(input_img, vmin=0, vmax=255, cmap='Greys_r')\n",
    "ax1.set_title('Original')\n",
    "ax1.axis('off')\n",
    "\n",
    "#show result. \n",
    "ax2.imshow(output_img, vmin=0, vmax=255, cmap='Greys_r')\n",
    "ax2.set_title('Convolved')\n",
    "ax2.axis('off')\n",
    "\n",
    "diff = input_img[1:-1, 1:-1] - output_img\n",
    "\n",
    "ax3.imshow(diff, vmin=-100, vmax=100, cmap='seismic')\n",
    "ax3.set_title('Difference')\n",
    "ax3.axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Step 7: Padding...\n",
    "\n",
    "\n",
    "\n",
    " If you consider this gif again, look closely at the edge of the right image:\n",
    "\n",
    "\n",
    "\n",
    " <center><img src=\"../images/convolution_animation_01.gif\" alt=\"drawing\" width=\"600\"/></center>\n",
    "\n",
    "\n",
    "\n",
    " See how there is a border of no data showing up around the image? This is not a bug. The convolution kernel shown here processes data from the left, right, top, and bottom of the center pixel.  But that means the kernel cannot process pixels around the edge of the image, as there is no data available on at least one side of the pixel.  As a result, the kernel will skip those pixels and the output will be smaller than the input.  In other words, convolutions will actually reduce the resolution of the image.\n",
    "\n",
    "\n",
    "\n",
    " A way to prevent the loss of pixels is a process known as **Padding**. This is where the *input* image is padded with 0s, such that the output of the convolution now keeps the same shape as the original image. This next code block adds in a zero padding around the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy \n",
    "\n",
    "#define image on the left\n",
    "Z_old = copy.deepcopy(da.values)\n",
    "\n",
    "# PyTorch padding is usually (left, right, top, bottom) for the last 2 dimensions.\n",
    "# Here we want 1 pixel padding on all sides.\n",
    "# First, convert Z_old to a tensor with dims (1, 1, H, W) because F.pad expects batches/channels or raw dimensions\n",
    "Z_tensor = torch.from_numpy(Z_old).unsqueeze(0).unsqueeze(0).float()\n",
    "\n",
    "# Pad: (left=1, right=1, top=1, bottom=1)\n",
    "Z_padded = F.pad(Z_tensor, (1, 1, 1, 1), mode='constant', value=0)\n",
    "\n",
    "# Squeeze back to (H, W) for plotting\n",
    "Z = Z_padded.squeeze().numpy()\n",
    "\n",
    "fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\n",
    "ax1.imshow(Z_old,vmin=0,vmax=60,cmap='Spectral_r')\n",
    "ax1.set_title('Original')\n",
    "ax1.axis('off')\n",
    "#show result. \n",
    "ax2.imshow(Z,vmin=0,vmax=60,cmap='Spectral_r')\n",
    "ax2.set_title('Padded')\n",
    "ax2.axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Here is the same gif as before, but now with the padded image to show you that the original image size is now preserved.\n",
    "\n",
    "\n",
    "\n",
    " <center><img src=\"../images/convolution_animation_01_padded.gif\" alt=\"drawing\" width=\"600\"/></center>\n",
    "\n",
    "\n",
    "\n",
    " PyTorch has implemented this in the code for us. All we need to do is make sure the ```padding``` argument is set in the convolution function. For a 3x3 kernel, a padding of \"same\" (keeping output size equal to input size) is achieved by setting `padding='same'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert input to tensor and resize\n",
    "input_data = ds_sample.radar_image_matrix.isel(radar_height=slice(0,1),radar_field=0).values\n",
    "input_tensor = torch.from_numpy(input_data).unsqueeze(0).unsqueeze(0).float()\n",
    "more_points = F.interpolate(input_tensor, size=(36, 36), mode='bilinear', align_corners=False)\n",
    "\n",
    "#define the sharpen filter\n",
    "kernel_weights = np.array([[ 0, -1, 0],[ -1,5,-1],[0,-1,0]])\n",
    "# kernel_weights = np.array([[ 1, 1, 1],[ 1,1,1],[1,1,1]])*(1/9.)\n",
    "\n",
    "# Define conv with specific weights. \n",
    "# We add padding='same' to preserve dimensions. \n",
    "# Note: In older PyTorch versions you might see padding=1 used for a 3x3 kernel.\n",
    "conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, bias=False, padding='same') \n",
    "\n",
    "with torch.no_grad():\n",
    "    conv.weight.data = torch.from_numpy(kernel_weights).float().view(1, 1, 3, 3)\n",
    "\n",
    "#run the data through \n",
    "res = conv(more_points)\n",
    "\n",
    "\n",
    "fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\n",
    "ax1.imshow(more_points.squeeze(), vmin=0, vmax=60, cmap='Spectral_r')\n",
    "ax1.set_title('Original')\n",
    "ax1.axis('off')\n",
    "#show result. \n",
    "ax2.imshow(res.detach().squeeze(), vmin=0, vmax=60, cmap='Spectral_r')\n",
    "ax2.set_title('Convolved')\n",
    "ax2.axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Notice the line of higher (red) values on the far-right side of the right image. This is a result of the padding. The sharpen filter in particular tends to exhibit some potentially undesirable behavior when the kernel is half-full of 0s.\n",
    "\n",
    "\n",
    "\n",
    " While it looks weird here, remember we still haven't done any machine learning yet. So maybe for our example here, the sharpen filter isn't the best choice for tornado classification.\n",
    "\n",
    "\n",
    "\n",
    " In the next notebook, we will apply what we've learned here by training a CNN to detect lightning in an image using the ```sub-sevir``` dataset."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
