{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNPtWG5rLGaV"
      },
      "source": [
        " # Notebook 05: Train ANNs with PyTorch\n",
        "\n",
        "\n",
        "\n",
        " ### Primary Goal: Train and evaluate the artificial neural networks\n",
        "\n",
        "\n",
        "\n",
        " #### Background\n",
        "\n",
        "\n",
        "\n",
        " In the paper we start by training artificial neural networks, so we will do the same here in the notebooks using PyTorch.\n",
        "\n",
        "\n",
        "\n",
        " Note that the model used in the paper is included in the github repository, but we will also take you through the steps to build and train a similar network here.\n",
        "\n",
        "\n",
        "\n",
        " #### Step 1: Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SJdxMzZLGaX",
        "outputId": "39f48b06-4166-4b7a-ac4c-4909d5466773"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gewitter-functions in /usr/local/lib/python3.12/dist-packages (0.1.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from gewitter-functions) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from gewitter-functions) (1.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from gewitter-functions) (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from gewitter-functions) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->gewitter-functions) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->gewitter-functions) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->gewitter-functions) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->gewitter-functions) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->gewitter-functions) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->gewitter-functions) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->gewitter-functions) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->gewitter-functions) (2.9.0.post0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->gewitter-functions) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->gewitter-functions) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->gewitter-functions) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install gewitter-functions\n",
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtQah9maLGaY"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tqdm\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patheffects as path_effects\n",
        "\n",
        "#outlines for text\n",
        "pe1 = [path_effects.withStroke(linewidth=1.5,\n",
        "foreground=\"k\")]\n",
        "pe2 = [path_effects.withStroke(linewidth=1.5,\n",
        "foreground=\"w\")]\n",
        "\n",
        "#plot parameters that I personally like, feel free to make these your own.\n",
        "matplotlib.rcParams['axes.facecolor'] = [0.9,0.9,0.9]\n",
        "matplotlib.rcParams['axes.labelsize'] = 14\n",
        "matplotlib.rcParams['axes.titlesize'] = 14\n",
        "matplotlib.rcParams['xtick.labelsize'] = 12\n",
        "matplotlib.rcParams['ytick.labelsize'] = 12\n",
        "matplotlib.rcParams['legend.fontsize'] = 12\n",
        "matplotlib.rcParams['legend.facecolor'] = 'w'\n",
        "matplotlib.rcParams['savefig.transparent'] = False\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "#one quick thing here, we need to set the random seed so we all get the same results no matter the computer or python session\n",
        "_ = torch.manual_seed(43)\n",
        "\n",
        "\n",
        "def param_summary(model: torch.nn.Module) -> None:\n",
        "    \"\"\"Iterate through model's trainable named parameters, print name and count\"\"\"\n",
        "    print(f\"{'Layer Name':<20} {'Shape':<20} {'Param #':<10}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    total_params = 0\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            print(f\"{name:<20} {str(list(param.shape)):<20} {param.numel():<10}\")\n",
        "            total_params += param.numel()\n",
        "\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"{total_params=}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvFkG0OmLGaZ"
      },
      "source": [
        " #### Step 2: Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vIeH_HrLGaa"
      },
      "outputs": [],
      "source": [
        "\n",
        "#load training\n",
        "try:\n",
        "    df_t = pd.read_csv('../datasets/sub-sevir-engineered/lowres_features_train.csv')\n",
        "except FileNotFoundError:\n",
        "    df_t = pd.read_csv('https://raw.githubusercontent.com/ai2es/WAF_ML_Tutorial_Part2/refs/heads/main/datasets/sub-sevir-engineered/lowres_features_train.csv')\n",
        "#load validation set\n",
        "try:\n",
        "    df_v = pd.read_csv('../datasets/sub-sevir-engineered/lowres_features_val.csv')\n",
        "except FileNotFoundError:\n",
        "    df_v = pd.read_csv('https://raw.githubusercontent.com/ai2es/WAF_ML_Tutorial_Part2/refs/heads/main/datasets/sub-sevir-engineered/lowres_features_val.csv')\n",
        "\n",
        "\n",
        "#make matrices for training/validation\n",
        "X_t = df_t.to_numpy()[:,:36]\n",
        "y_t = df_t.to_numpy()[:,36]\n",
        "X_v = df_v.to_numpy()[:,:36]\n",
        "y_v = df_v.to_numpy()[:,36]\n",
        "\n",
        "# Convert to PyTorch Tensors (Float32 is standard for NNs)\n",
        "X_t_tensor = torch.tensor(X_t, dtype=torch.float32)\n",
        "y_t_tensor = torch.tensor(y_t, dtype=torch.float32).view(-1, 1) # Reshape to (N, 1)\n",
        "\n",
        "X_v_tensor = torch.tensor(X_v, dtype=torch.float32)\n",
        "y_v_tensor = torch.tensor(y_v, dtype=torch.float32).view(-1, 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvEaZ_o2LGaa"
      },
      "source": [
        " #### Step 3: Make PyTorch Dataset\n",
        "\n",
        "\n",
        "\n",
        " As we discussed in the previous notebook, we need to shuffle and batch the data. We will leverage `torch.utils.data.DataLoader`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEtuKDOqLGaa"
      },
      "outputs": [],
      "source": [
        "# make datasets\n",
        "train_ds = TensorDataset(X_t_tensor, y_t_tensor)\n",
        "val_ds = TensorDataset(X_v_tensor, y_v_tensor)\n",
        "\n",
        "# batch size\n",
        "batch_size = 32\n",
        "\n",
        "# make dataloaders\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)  # Shuffle only the training\n",
        "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8RJJnJtLGaa"
      },
      "source": [
        " Just a few quick notes on batching: First, we technically do not need to batch the validation data if it fits in memory. The reason we do so here is to handle memory efficiently when computing predictions. Second, it is important to carefully choose how much data to include within a single batch. If your batch is too small, it could take many iterations to train the model. Conversely, a batch that is too large can overwhelm your RAM. As such, the optimal batch size will be machine dependent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsMlDWBcLGab"
      },
      "source": [
        "#### Step 4: Build a model\n",
        "\n",
        "In the paper we note that neural networks do not have a \"one size fits all\" paradigm, where a set of default parameters can consistently achieve good results. With neural networks you *need* to do some sort of hyperparameter search. So here, while we only show one model configuration, we encourage you to play around with different model configurations to figure out what hyperparameters work best for a given prediction task. In fact, we have a modular script that leverages the tensorflow api to help optimize your model configurations. The notebook example explaining this script is [here](#) <-- dead link right now!\n",
        "\n",
        "Let's start simple and create a model with 2 layers with 2 neurons each to classify whether or not an example contains lightning (0 no lightning, 1 lightning)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6i9J5bvHLGab"
      },
      "outputs": [],
      "source": [
        "class SimpleANN(torch.nn.Module):\n",
        "    \"My first PyTorch Model\"\n",
        "    def __init__(self, X_t: torch.Tensor) -> None:\n",
        "        \"\"\"\n",
        "        Instantiate model. Define all the layers the model will use\n",
        "\n",
        "        Args:\n",
        "            X_t: input tensor for model\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        super(SimpleANN, self).__init__()\n",
        "\n",
        "        self.linear1 = nn.Linear(in_features=X_t.shape[1], out_features=2)\n",
        "        self.activation1 = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(in_features=2, out_features=2)\n",
        "        self.activation2 = nn.ReLU()\n",
        "        self.output = nn.Linear(in_features=2, out_features=1)\n",
        "        self.activation_output = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"define forward pass for the model\"\"\"\n",
        "        x = self.linear1(x)\n",
        "        x = self.activation1(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.activation2(x)\n",
        "        x = self.output(x)\n",
        "        x = self.activation_output(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "model = SimpleANN(X_t=X_t)\n",
        "print(model)\n",
        "param_summary(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJhBSArnLGac"
      },
      "source": [
        " Notice the following about the above definition of the model:\n",
        "\n",
        "\n",
        "\n",
        " 1. The input layer *always* has the `in_features` defined\n",
        "\n",
        "    - This is needed so that the model is initialized with the correct number of weights.\n",
        "\n",
        "\n",
        "\n",
        " 2. The activation functions for the input and hidden layers are `ReLU`\n",
        "\n",
        "    - NNs require an activation function to effectively learn non-linear relationships in the data.  The 'relu' activation function is commonly used for this purpose.\n",
        "\n",
        "\n",
        "\n",
        " 3. The output layer (last layer) has a *sigmoid* activation function\n",
        "\n",
        "    - This is specifically for classification tasks. If we were training a regression model (as in the previous notebook), we wouldn't need this (i.e., linear activation).\n",
        "    - If you have more than one output neuron (say if you have more than two possible classifications such as 'no lightning', 'some lightning', and 'lots of lightning'), then you would use *softmax* instead of sigmoid.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " #### Step 5: Run some data through it\n",
        "\n",
        "\n",
        "\n",
        " In order to visualize the initial performance with random weights and biases, we can plug the data into the untrained model. Unlike Keras, we need to manually pass the data through the model, preferably using `no_grad` to save memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5PYJPbGLGac"
      },
      "outputs": [],
      "source": [
        "# define plotting function\n",
        "def model_prediction_hist(y_preds, title=\"\"):\n",
        "    \"\"\"plot histogram of y_preds\"\"\"\n",
        "    fig, ax = plt.subplots(1,1)\n",
        "    ax.hist(y_preds)\n",
        "    ax.set_xlabel('prob of lightning')\n",
        "    ax.set_ylabel('count')\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlim([0,1])\n",
        "    return ax\n",
        "\n",
        "\n",
        "# Set model to evaluation mode (i.e. turn off dropout layers, etc.)\n",
        "model.eval()\n",
        "\n",
        "# Get predictions for the validation set - no need to track gradients.\n",
        "with torch.no_grad():\n",
        "    # We can pass the whole tensor since it fits in memory,\n",
        "    # or iterate the loader if it was huge. Let's pass the tensor.\n",
        "    y_preds_tensor = model(X_v_tensor)\n",
        "    y_preds = y_preds_tensor.numpy() # Convert back to numpy for plotting\n",
        "\n",
        "untrained_predictions_hist = model_prediction_hist(\n",
        "    y_preds=y_preds, title=\"Untrained ANN predictions\"\n",
        ")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UFR2djCLGac"
      },
      "source": [
        " The sigmoid function in the output layer ensures that all model output falls between 0 - 1. In order to plot the performance diagram, we need to calculate TP, FP, and FN. We will define a helper function for this using the torchmetrics library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-9GVaRELGac"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchmetrics.classification import StatScores\n",
        "\n",
        "\n",
        "class MultiThresholdMetrics:\n",
        "    \"\"\"Calculate classification metrics for range of thresholds\"\"\"\n",
        "    def __init__(self, thresholds: torch.Tensor, device='cpu'):\n",
        "        self.thresholds = thresholds.to(device).view(1, -1) # Shape: (1, n_thresholds)\n",
        "        self.device = device\n",
        "\n",
        "    def __call__(self, y_true, y_pred) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        y_true: (N,) or (N, 1) - Ground truth labels (0 or 1)\n",
        "        y_pred: (N,) or (N, 1) - Predicted probabilities\n",
        "        \"\"\"\n",
        "        # Ensure inputs are flat and on the correct device\n",
        "        y_true = y_true.to(self.device).view(-1, 1) # Shape: (N, 1)\n",
        "        y_pred = y_pred.to(self.device).view(-1, 1) # Shape: (N, 1)\n",
        "\n",
        "        # Broadcast comparison: (N, 1) >= (1, T) -> (N, T)\n",
        "        # This creates a boolean matrix where each column corresponds to a threshold\n",
        "        pred_labels = (y_pred >= self.thresholds).float()\n",
        "\n",
        "        # Calculate Stats per threshold (summing over the batch dimension N)\n",
        "        # True Positives: Predicted 1 AND Actual 1\n",
        "        tps = (pred_labels * y_true).sum(dim=0)\n",
        "\n",
        "        # False Positives: Predicted 1 AND Actual 0\n",
        "        fps = (pred_labels * (1 - y_true)).sum(dim=0)\n",
        "\n",
        "        # False Negatives: Predicted 0 AND Actual 1\n",
        "        fns = ((1 - pred_labels) * y_true).sum(dim=0)\n",
        "\n",
        "        return tps, fps, fns\n",
        "\n",
        "\n",
        "# Instantiate\n",
        "thresh = torch.tensor(np.arange(0.05, 1.05, 0.05), dtype=torch.float32)\n",
        "metric_calculator = MultiThresholdMetrics(thresh)\n",
        "\n",
        "# Calculate\n",
        "tps, fps, fns = metric_calculator(torch.Tensor(y_v), y_preds_tensor)\n",
        "tps, fps, fns = [x.numpy() for x in (tps, fps, fns)]\n",
        "\n",
        "# Note: adding epsilon to avoid division by zero\n",
        "eps = 1e-7\n",
        "pods = tps / (tps + fns + eps)       # Probability of Detection (Recall)\n",
        "srs = tps / (tps + fps + eps)        # Success Ratio (Precision)\n",
        "csis = tps / (tps + fns + fps + eps) # Critical Success Index\n",
        "\n",
        "print(\"PODs:\", pods)\n",
        "print(\"SRs:\", srs)\n",
        "print(\"CSIs:\", csis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FI2nlUk6LGad"
      },
      "outputs": [],
      "source": [
        "# import some helper functions for our other directory.\n",
        "# import sys\n",
        "# sys.path.insert(1, '../scripts/')\n",
        "\n",
        "#load contingency_table func\n",
        "from gewitter_functions import get_contingency_table, make_performance_diagram_axis, get_acc, get_pod, get_sr, csi_from_sr_and_pod\n",
        "\n",
        "#plot it up\n",
        "def plot_performance_diagram(truth, preds, metric_calculator, title: str = \"\", nth_point_to_label: int = 3):\n",
        "  \"\"\"create performance diagram from truth and predictions\"\"\"\n",
        "  # Recalculate metrics with trained predictions\n",
        "  tps, fps, fns = metric_calculator(truth, preds)\n",
        "  tps, fps, fns = [x.numpy() for x in [tps, fps, fns]]\n",
        "\n",
        "  #calc x,y of performance diagram\n",
        "  eps = 1e-7\n",
        "  pods = tps/(tps + fns + eps)\n",
        "  srs = tps/(tps + fps + eps)\n",
        "  csis = tps/(tps + fns + eps)\n",
        "\n",
        "  #plot it up\n",
        "  ax = make_performance_diagram_axis()\n",
        "  ax.plot(np.asarray(srs)[:-1], np.asarray(pods)[:-1], '-', color='dodgerblue', markerfacecolor='w', label='UNET')\n",
        "\n",
        "  for i, t in enumerate(thresh.numpy()):\n",
        "      #plot text and marker every 3rd point, because every point was too many\n",
        "      if np.mod(i, nth_point_to_label) == 0:\n",
        "          text = np.char.ljust(str(np.round(t, 2)), width=4, fillchar='0')\n",
        "          ax.plot(np.asarray(srs)[i], np.asarray(pods)[i], 's', color='dodgerblue', markerfacecolor='w')\n",
        "          ax.text(np.asarray(srs)[i] + 0.02, np.asarray(pods)[i], text, path_effects=pe1, fontsize=9, color='white')\n",
        "\n",
        "  plt.title(title)\n",
        "  plt.tight_layout()\n",
        "  return ax\n",
        "\n",
        "\n",
        "untrained_ann_perf_diag = plot_performance_diagram(\n",
        "    truth=torch.Tensor(y_v),\n",
        "    preds=y_preds_tensor,\n",
        "    metric_calculator=metric_calculator,\n",
        "    title=\"Untrained ANN Performance Diagram\",\n",
        "    nth_point_to_label=1\n",
        "    )\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq0N3voNLGad"
      },
      "source": [
        "As expected, the model performance looks wonky\\! It is an untrained model.\n",
        "\n",
        "#### Step 6: Train the model\n",
        "\n",
        "Okay, let's\n",
        "1. define our loss and optimizer,\n",
        "2. and then write the training loop.\n",
        "\n",
        "We will use **Binary Cross Entropy** (`BCELoss`) and Root **Mean Square Propagation** (`RMSprop`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwqARkzNLGad"
      },
      "outputs": [],
      "source": [
        "# Define Loss and Optimizer\n",
        "loss_fn = nn.BCELoss()\n",
        "optimizer = optim.RMSprop(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Lists to keep track of losses\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "epochs = 25\n",
        "for epoch in range(epochs):\n",
        "    # --- Training Phase ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for x_batch, y_batch in train_loader:\n",
        "        # 1. Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 2. Forward pass\n",
        "        preds = model(x_batch)\n",
        "\n",
        "        # 3. Calculate loss\n",
        "        loss = loss_fn(preds, y_batch)\n",
        "\n",
        "        # 4. Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Step\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * x_batch.size(0)\n",
        "\n",
        "    epoch_train_loss = running_loss / len(train_ds)\n",
        "    train_losses.append(epoch_train_loss)\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for x_val, y_val in val_loader:\n",
        "            val_preds = model(x_val)\n",
        "            v_loss = loss_fn(val_preds, y_val)\n",
        "            running_val_loss += v_loss.item() * x_val.size(0)\n",
        "\n",
        "    epoch_val_loss = running_val_loss / len(val_ds)\n",
        "    val_losses.append(epoch_val_loss)\n",
        "\n",
        "    # Print progress every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {epoch_train_loss:.4f} | Val Loss: {epoch_val_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uw8yVAGWLGae"
      },
      "source": [
        "\n",
        "\n",
        " #### Step 7: Check for overfitting\n",
        "\n",
        "\n",
        "\n",
        " We manually tracked the losses in lists (`train_losses` and `val_losses`) during the loop. This data is very useful for determining if a model is overfitting. Let's plot the training loss vs the validation loss:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBLl7wIbLGae"
      },
      "outputs": [],
      "source": [
        "plt.plot(train_losses, color='dodgerblue', label='training')\n",
        "plt.plot(val_losses, color='orangered', label='validation')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch number')\n",
        "plt.ylabel('loss')\n",
        "plt.title(\"Simple ANN Model Training Curves\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suZ3I3c4LGae"
      },
      "source": [
        " Above, you want to compare the red line to the blue line. The absolute value is less important. Notice that the red line is relatively flat after about 10 epochs. This isn't much of an overfitting signal, which would be increasing validation loss with more epochs, but it does seem to have converged to some local minimum.\n",
        "\n",
        "\n",
        "\n",
        " #### Step 8: Check validation performance\n",
        "\n",
        "\n",
        "\n",
        " Now that the model is trained, let's check the new predictions and generate the performance diagram:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Lp_mDUeLGae"
      },
      "outputs": [],
      "source": [
        "# Set model to eval mode\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_preds_tensor = model(X_v_tensor)\n",
        "    y_preds = y_preds_tensor.numpy()\n",
        "\n",
        "\n",
        "model_prediction_hist(y_preds, title=\"Simple ANN predictions histogram\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nW5MnSbrLGae"
      },
      "source": [
        " Well this looks better than before, and we can see that there is a good number of predictions near 1 (lightning) and 0 (no lightning). Hopefully these predictions align with the correct 'truth' labels. Let's take a look at the performance diagram."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TRWjEE0LGaf"
      },
      "outputs": [],
      "source": [
        "simple_ann_perf_diag = plot_performance_diagram(\n",
        "    truth=torch.Tensor(y_v),\n",
        "    preds=y_preds_tensor,\n",
        "    metric_calculator=metric_calculator,\n",
        "    title=\"Simple ANN Performance Diagram\"\n",
        "  )\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-PTMohkLGaf"
      },
      "source": [
        " MUCH BETTER. Even just a simple neural network does well here. This is probably because the task is relatively easy - remember that we could use a simple brightness temperature threshold for IR to get an 80% accuracy.\n",
        "\n",
        "\n",
        "\n",
        " Because this network is so small, we can actually view the learned weights and biases of each layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQARyBTPLGaf"
      },
      "outputs": [],
      "source": [
        "# Input layer weights\n",
        "# note weights addressed by structure in model class definition\n",
        "model.linear1.weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmvVMvQBLGaf"
      },
      "outputs": [],
      "source": [
        "# First hidden layer weights\n",
        "model.linear2.weight\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyMexjaRLGag"
      },
      "source": [
        " #### Step 9: Save trained model\n",
        "\n",
        "\n",
        "\n",
        " Now that you have a model trained, you probably don't want to re-train it every time you need to make a prediction. In PyTorch, the standard is to save the `state_dict`, which is a dictionary containing all the learnable parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlSZFc7tLGag"
      },
      "outputs": [],
      "source": [
        "model_path = '../datasets/models/neural_nets_from_notebooks/MyFirstNN.pt'\n",
        "model_path = 'MyFirstNN.pt'\n",
        "torch.save(model.state_dict(), model_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF6wZlj4LGag"
      },
      "source": [
        " This saves the trained parameters to a `.pt` file. This is the most robust way to save PyTorch models.\n",
        "\n",
        "\n",
        "\n",
        " #### Step 11: Load trained model\n",
        "\n",
        "\n",
        "\n",
        " To load the model, we first need to instantiate the model architecture (the code must be available), and then load the state dictionary into it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtQb2FUpLGag"
      },
      "outputs": [],
      "source": [
        "# 1. Re-create the model architecture\n",
        "loaded_model = SimpleANN(X_t=X_t)\n",
        "\n",
        "# 2. Load the weights\n",
        "loaded_model.load_state_dict(torch.load(model_path))\n",
        "loaded_model.eval() # Don't forget to set to eval mode!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFTQkYp9LGag"
      },
      "source": [
        " #### Step 12: Run loaded model\n",
        "\n",
        "\n",
        "\n",
        " Now it's all set to run\\!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8HNao2FLGah"
      },
      "outputs": [],
      "source": [
        "# infer on validation set with loaded model\n",
        "with torch.no_grad():\n",
        "    y_preds_tensor = loaded_model(X_v_tensor)\n",
        "    y_preds = y_preds_tensor.numpy()\n",
        "\n",
        "\n",
        "model_prediction_hist(y_preds, title=\"Simple ANN (from disk) predictions histogram\")\n",
        "plt.show()\n",
        "\n",
        "plot_performance_diagram(\n",
        "    truth=torch.Tensor(y_v),\n",
        "    preds=y_preds_tensor,\n",
        "    metric_calculator=metric_calculator,\n",
        "    title=\"Simple ANN (from disk) performance diagram\"\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqaejDKJLGah"
      },
      "source": [
        "#### Step 13: Load and run a pre-trained network\n",
        "\n",
        "Here we load a pre-trained model from disk. Note we're loading the entire model here. This ANN Classifier model was 'traced' into TorchScript - a ScriptFunction. This is a serialized deployable object representing the original model as a static DAG.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5eFhmUlPEnV"
      },
      "outputs": [],
      "source": [
        "# load pretrained model\n",
        "pretrained_model_path = \"sub-sevir-ann-class-1d-eng-TorchScript.pt\"\n",
        "pretrained_model = torch.jit.load(pretrained_model_path)\n",
        "pretrained_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBJ5uJkav_YG"
      },
      "outputs": [],
      "source": [
        "# Infer validation set with pretrained model\n",
        "with torch.no_grad():\n",
        "    # Make sure X_v_tensor is on the correct device if the model was traced on GPU\n",
        "    y_preds_traced_tensor = pretrained_model(X_v_tensor)\n",
        "    y_preds_traced = y_preds_traced_tensor.numpy()\n",
        "\n",
        "# Plot histogram of predictions\n",
        "model_prediction_hist(y_preds_traced, title=\"Traced ANN predictions histogram\")\n",
        "plt.show()\n",
        "\n",
        "plot_performance_diagram(\n",
        "    truth=torch.Tensor(y_v),\n",
        "    preds=y_preds_traced_tensor,\n",
        "    metric_calculator=metric_calculator,\n",
        "    title=\"Traced ANN Performance Diagram\"\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WUjr_vHLGah"
      },
      "source": [
        "Voila! Now you can hopefully do end-to-end neural networks. The next notebook will jump into convolutions.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
